# LLM

- LLM: Large Language Model (Cloude, OpenAI, Gemini, Llama, Mistral, etc.)
- SML: Small Language Model

- **Modelo**: RepresentaÃ§Ã£o da realidade
- **Linguagem**: Objeto do modelo Ã© a linguagem
- **Large**: BilhÃµes de parÃ¢metros (pesos e vieses)
- **Generativo**: Capacidade de gerar novos dados.

## GPT

- GPT: Generative Pre-trained Transformer

- **Modelo Unigrama**: NÃ£o considera o contexto, apenas a frequÃªncia das palavras. O que voce escreve nÃ£o impacta na probabilidade da prÃ³xima palavra.

- **Modelo Bi-grama**: Considera a palavra anterior para prever a prÃ³xima palavra. O que voce escreve impacta na probabilidade da prÃ³xima palavra.

- **Modelo N-grama**: Considera as N palavras anteriores para prever a prÃ³xima palavra. O que voce escreve impacta na probabilidade da prÃ³xima palavra. (Os LLMs sÃ£o modelos N-grama com N muito grande, o que permite considerar um contexto maior).

- **Token**: Unidade bÃ¡sica de texto que o modelo processa. Pode ser uma palavra, parte de uma palavra ou atÃ© mesmo um caractere, dependendo do modelo.

- **Embedding**: RepresentaÃ§Ã£o numÃ©rica de palavras ou frases em um espaÃ§o vetorial, permitindo que o modelo compreenda relaÃ§Ãµes semÃ¢nticas entre elas.

- **Stemming**: Processo de reduzir palavras Ã s suas raÃ­zes ou formas bÃ¡sicas, removendo sufixos e prefixos. (lematizaÃ§Ã£o)

Todo modelo de linguagem tem Tokens e Stemming por trÃ¡s.


## Formula de Bayes

Probabildidade = Chance de um evento ocorrer

```plaintext
P(A) = # OcorrÃªncias de A / # Total de Eventos (espaÃ§o amostral)
```

- P(A,B) -> Probabilidade conjunta de A e B ocorrerem juntos (A e B)
- P(A|B) -> Probabilidade condicional de A ocorrer dado que B ocorreu (A dado B)

```plaintext
P(A,B) = P(A|B) * P(B) -> # A e B ocorrem / # Total de eventos

P(A|B) = P(A,B) / P(B) -> # A e B ocorrem / # onde B ocorre

P(A|B) = P(B|A) / P(B) * P(A) -> # A e B ocorrem / # onde B ocorre

P(B|A) / P(B) -> Permite atualizar o conhecimento da nova informaÃ§Ã£o. Ã‰ o aprendizado (learning) do modelo.
```

### AplicaÃ§Ãµes

- Pela refelexÃ£o de um espectro eletromagnÃ©tico, podemos inferir a probabilidade de um peixe ser salmÃ£o ou atum.
- Pela anÃ¡lise de um texto, podemos inferir a probabilidade de um email ser spam ou nÃ£o spam.
- Pela anÃ¡lise de sintomas, podemos inferir a probabilidade de um paciente ter uma doenÃ§a especÃ­fica

Para plotar grÃ¡ficos: https://desmos.com/calculator

### O Modelo de Naive Bayes

O modelo de Naive Bayes Ã© um classificador probabilÃ­stico baseado no Teorema de Bayes, que assume a independÃªncia entre as caracterÃ­sticas (features) do conjunto de dados. Apesar dessa suposiÃ§Ã£o simplista, o modelo Ã© eficaz em vÃ¡rias aplicaÃ§Ãµes prÃ¡ticas, como filtragem de spam, anÃ¡lise de sentimentos e reconhecimento de texto. 

- Ã‰ um modelo de classificaÃ§Ã£o.

- **IndependÃªncia de variÃ¡veis**: Uma variÃ¡vel nÃ£o influencia a outra. Exemplo: A cor do carro nÃ£o influencia o modelo do carro. `P(A,B) = P(A) * P(B)`. Se A e B sÃ£o independentes, a probabilidade de A dado B Ã© igual a probabilidade de A. `P(A|B) = P(A)`.

- Modelos servem para simplificar a realidade.
- O modelo de Naive Bayes supÃµe que as variÃ¡veis wi possuem uma independÃªncia condicional, ou seja, a probabilidade de wi dado a classe c Ã© independente das outras variÃ¡veis. `P(wi|c, wj) = P(wi|c)`.
- O Naive Bayes permite calcular e estimar eventos que nÃ£o foram observados no conjunto de dados de treinamento.


#### Detector de SPAM

1. Carrega csv (messagens classificadas como spam ou ham)
2. Tokeniza as frases (separa em tokens)
3. Para melhorar a performance, aplica Stemming (reduz as palavras Ã s suas raÃ­zes)
4. Calcula o logaritmo da probabilidade de cada frase ser spam ou ham.
5. Classifica novas mensagens com base nas probabilidades calculadas.

## ClassificaÃ§Ã£o e RegressÃ£o

- **Aprendizado Supervisionado**: PrevisÃ•es -> dados -> categorizados (labeled). Dado um conjunto de dados com entradas e saÃ­das conhecidas, o modelo aprende a mapear as entradas para as saÃ­das corretas. Exemplo: ClassificaÃ§Ã£o de emails como spam ou nÃ£o spam, reconhecimento de imagens, etc. Nosso cerebro funciona diferente (fewshot learning e reinforcement learning).

- **ClassificaÃ§Ã£o**: Categorias discretas (spam ou nÃ£o spam, gato ou cachorro, etc.) Ex. ClassificaÃ§Ã£o logÃ­stica, SVM, Decision Trees, Random Forest, etc.
- **RegressÃ£o**: Valores contÃ­nuos (preÃ§o de uma casa, temperatura, etc.)Ex. RegressÃ£o linear, regressÃ£o polinomial, etc.

## O que Ã© o Gen de GenAI?

GenAI refere-se a uma classe de modelos de inteligÃªncia artificial projetados para gerar texto, imagens ou outros tipos de conteÃºdo de forma autÃ´noma. Esses modelos sÃ£o treinados em grandes conjuntos de dados e utilizam tÃ©cnicas avanÃ§adas de aprendizado de mÃ¡quina para criar conteÃºdo novo e original com base em padrÃµes aprendidos durante o treinamento.

- **Modelo Discriminativo**: Classifica ou rotula os dados de entrada. Exemplo: ClassificaÃ§Ã£o de emails como spam ou nÃ£o spam.
- **Modelo Generativo**: Gera novos dados semelhantes aos dados de treinamento. Exemplo: GeraÃ§Ã£o de texto, imagens, etc.

- **Deep Learning**: Subcampo do aprendizado de mÃ¡quina que utiliza redes neurais profundas para modelar e resolver problemas complexos. Exemplo: Redes neurais convolucionais (CNNs) para reconhecimento de imagens, redes neurais recorrentes (RNNs) para processamento de linguagem natural, etc.
- **GAN**: Generative Adversarial Network. Consiste em duas redes neurais que competem entre si: uma geradora (generator) que cria novos dados e uma discriminadora (discriminator) que avalia a autenticidade dos dados gerados. O objetivo Ã© melhorar a qualidade dos dados gerados ao longo do tempo.
- **VAE**: Variational Autoencoder. Ã‰ um tipo de rede neural que aprende a codificar dados de entrada em um espaÃ§o latente e, em seguida, decodifica esse espaÃ§o latente para gerar novos dados semelhantes aos dados de entrada originais.

## RAG (Retrieval-Augmented Generation)

RAG Ã© uma tÃ©cnica que combina modelos de linguagem generativos com sistemas de recuperaÃ§Ã£o de informaÃ§Ãµes para melhorar a precisÃ£o e relevÃ¢ncia das respostas geradas. Em vez de depender exclusivamente do conhecimento prÃ©-treinado do modelo, o RAG utiliza uma base de dados externa para buscar informaÃ§Ãµes relevantes em tempo real, que sÃ£o entÃ£o incorporadas na geraÃ§Ã£o da resposta.

O RAG Ã© uma estratÃ©gia de contexto: Outra estratÃ©gia Ã© o fine-tuning, porÃ©m Ã© mais custoso e demorado.

RAG: Pergunta -> Busca a informaÃ§Ã£o relevante (Information Retrieval) -> Faz o prompt mais o contexto encontrado -> LLM Usa o contexto para gerar a resposta.

- **Bag of Words (BoW)**: RepresentaÃ§Ã£o de texto onde a ordem das palavras Ã© ignorada e apenas a frequÃªncia de cada palavra Ã© considerada. Cada documento Ã© representado como um vetor de contagem de palavras.
- **LematizaÃ§Ã£o**: Processo de reduzir palavras Ã s suas formas bÃ¡sicas ou dicionÃ¡rio, considerando o contexto. Exemplo: "correndo", "correu" e "correr" sÃ£o reduzidos a "correr".

N Documentos -> N Bags of Words -> N Vetores

- **TF**: Term Frequency. Mede a frequÃªncia de uma palavra em um documento. 
- **IDF**: Inverse Document Frequency. Mede a importÃ¢ncia de uma palavra em todo o conjunto de documentos. Palavra aparece muito em um documento, mas pouco em outros documentos, Ã© uma palavra importante.

Um bom information retrieval Ã© de suma importÃ¢ncia para o RAG.

Problema que RAG visa resolver: Encontrar a informaÃ§Ã£o correta em um grande volume de dados e gerar uma resposta precisa e relevante. Qual informaÃ§Ã£o melhor se adequa a query realizada?

## Embeddings

### Alebra Linear

- **Vetores**: RepresentaÃ§Ã£o numÃ©rica de palavras ou frases em um espaÃ§o vetorial, permitindo que o modelo compreenda relaÃ§Ãµes semÃ¢nticas entre elas.
- **Modulo**: Comprimento do vetor.
- **Produto Escalar**: Mede a similaridade entre dois vetores. Quanto maior o produto escalar, mais semelhantes sÃ£o os vetores.
- **Cosseno**: Mede o Ã¢ngulo entre dois vetores. Quanto menor o Ã¢ngulo, mais semelhantes sÃ£o os vetores.
- **DistÃ¢ncia Euclidiana**: Mede a distÃ¢ncia entre dois vetores. Quanto menor a distÃ¢ncia, mais semelhantes sÃ£o os vetores.
- **Matriz**: Conjunto de vetores organizados em linhas e colunas.
- **TransformaÃ§Ã£o Linear**: OperaÃ§Ã£o que transforma um vetor em outro vetor, preservando a estrutura linear. Exemplo: RotaÃ§Ã£o, translaÃ§Ã£o, escala, etc.
- **Autovalores e Autovetores**: Propriedades de uma matriz que descrevem como a matriz transforma os vetores. Autovetores sÃ£o vetores que nÃ£o mudam de direÃ§Ã£o apÃ³s a transformaÃ§Ã£o, apenas seu comprimento Ã© alterado pelo autovalor correspondente.

### One-Hot Vectors

- RepresentaÃ§Ã£o binÃ¡ria onde cada palavra Ã© representada por um vetor com um Ãºnico elemento "1" (indicando a presenÃ§a da palavra) e todos os outros elementos "0". Exemplo: Para um vocabulÃ¡rio de 5 palavras, a palavra "gato" pode ser representada como [0, 1, 0, 0, 0] para a frase "o gato estÃ¡ no telhado".

- Cada palavra Ã© uma direÃ§Ã£o diferente em um espaÃ§o de alta dimensÃ£o (ortogonais entre si).
- No One-Hot Vectors **Todas palavras sÃ£o ortogonais**.

- A distancia de todas as palavras Ã© a mesma (sqrt(2)). Por isso, todo conteÃºdo semÃ¢ntico Ã© perdido.
- NÃ£o captura relaÃ§Ãµes semÃ¢nticas entre palavras.
- Cada palavra Ã© uma dimensÃ£o diferente. EntÃ£o em textos longos, o espaÃ§o vetorial fica muito grande (curse of dimensionality) e se torna inviÃ¡vel.

Como capturar relaÃ§Ãµes semÃ¢nticas entre palavras?

- Queremos que palavras semelhantes estejam prÃ³ximas no espaÃ§o vetorial (theta ~ 0).
- Queremos que palavras diferentes estejam distantes no espaÃ§o vetorial (theta ~ 90 graus).
  
### Word2Vec

- TÃ©cnica de aprendizado de mÃ¡quina que transforma palavras em vetores densos, capturando relaÃ§Ãµes semÃ¢nticas entre elas.
- Utiliza redes neurais para aprender representaÃ§Ãµes vetoriais de palavras com base em seu contexto.
- Palavras semelhantes aparecem em contextos semelhantes, entÃ£o seus vetores devem estar prÃ³ximos no espaÃ§o vetorial.
- Dois modelos principais: Continuous Bag of Words (CBOW) e Skip-Gram.
- Skip-Gram: `O garoto jogou bola` -> Fixa a palavra garoto e olha as palavras no contexto (O, jogou, bola) para prever o vetor da palavra garoto.
- CBOW: `O garoto jogou bola` -> Fixa as palavras no contexto (O, jogou, bola) para prever o vetor da palavra garoto.
- Pega um espaÃ§o de altissima dimensÃ£o (todas as palavras do vocabulÃ¡rio) e reduz para um espaÃ§o de baixa dimensÃ£o (100, 200, 300 dimensÃµes).

Site com explicaÃ§Ã£o visual: 
- https://jalammar.github.io/illustrated-word2vec/
- https://projector.tensorflow.org/

OperaÃ§Ãµes com vetores:
```python
model.most_similar(positive=['king', 'woman'], negative=['man'])
# Resultado: queen
# rei - homem + mulher = rainha
```

### Sistema de memÃ³ria

```python

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

workflow = StateGraph(state_schema = MessagesState)

def call_mode(state: MessagesState):
  response = llm.invoke(state["messages"])
  return {"messages": response}

workflow.add_edge(START, "model")
workflow.add_node("model", call_mode)
memory = MemorySaver()
app = workflow.compile(memory)

query = "Oi! Meu nome Ã© Leo"
config = {"configurable": {"thread_id": "abc123"}}
inpute_messages = [HumanMessage(query)]
output = app.invoke({"messages": inpute_messages}, config)
output["messages"][-1].pretty_print()
# E aÃ­, Leo! Prazer em te conhecer. Em que posso te ajudar hoje? ðŸ˜Š
query = "Qual o meu nome?"
output = app.invoke({"messages": inpute_messages}, config)
output["messages"][-1].pretty_print()
# Seu nome Ã© Leo! VocÃª acabou de me dizer. ðŸ˜‰
config = {"configurable": {"thread_id": "xyz789"}}
output = app.invoke({"messages": inpute_messages}, config)
output["messages"][-1].pretty_print()
# Eu sou um modelo de linguagem grande, treinado pelo Google. Eu nÃ£o tenho nome.
``` 

## Fluxograma RAG

```mermaid
flowchart LR
    A[Pergunta] --> B[Banco de Dados]
    B --> C[PedaÃ§os Relevantes]
    C --> D[Pergunta + Contexto + prompt]
    A --> D
    D --> E[LLM]
    E --> F[Resposta]
```

No fundo Ã© um sistema de busca (search engine) + LLM.

- **Naive Rag:** Usa TF-IDF + Bag of Words (BoW) para buscar os pedaÃ§os relevantes no banco de dados.
- **Advanced Rag**: Usa Embeddings + Similaridade de Cosseno para buscar os pedaÃ§os relevantes no banco de dados.
- **Graph RAG**: Usa Grafos para buscar os pedaÃ§os relevantes no banco de dados.

## Structured Outputs

FormataÃ§Ã£o estruturada da resposta do modelo. Muito Ãºtil para extrair informaÃ§Ãµes especÃ­ficas e garantir que a resposta esteja em um formato consistente.

- **Pydantic**: Biblioteca de validaÃ§Ã£o de dados e configuraÃ§Ã£o baseada em tipos para Python. Utilizada para definir e validar estruturas de dados complexas, garantindo que os dados atendam a requisitos especÃ­ficos.

```python
from pydantic import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI
from google.colab import userdata
import os
import google.generativeai as genai
class Piada(BaseModel):
  setup: str = Field(description="A introduÃ§Ã£o da piada")
  punchline: str = Field(description="A parte engraÃ§ada da piada")
  rating: int = Field("AvaliaÃ§Ã£o de 1 a 10 da piada")

structured_llm = llm.with_structured_output(Piada)

repose = structured_llm.invoke("Me conte uma piada sobre gatos.")
print(repose.model_dump_json(indent=2))
# {
#   "setup": "Por que os gatos sÃ£o tÃ£o ruins no pÃ´quer?",
#   "punchline": "Porque sempre tem um Ã¡s na manga!",
#   "rating": 5
# }
```

## Tools

- As Tools nÃ£o sÃ£o chamadas pela LLM.
- Antigamente faziam tÃ©cnicas de prompts para a LLM construir o payload da API.
- MCP: Model-Centric Programming
- LLMS conseguem se comunicar com buscas, calculadoras, etc.
- A LLM nÃ£o chama a Tool, quem chama a Tool Ã© o programa.

```python
from langchain.tools import tool

@tool
def multiplique(a: float, b: float):
  """multipla dois numeros reais"""""
  print(f"Multiplicando {a} por {b}")
  return a * b

llm_with_tools = llm.bind_tools(multiplique)
result = llm_with_tools.invoke("Multiplique 1 por 2")
print(result)

tool_call = result.tool_calls[0]
tool_name = tool_call['name']
tool_args = tool_call['args']
print(tool_name)
print(tool_args)

if tool_name == multiplique.name:
    result = multiplique.invoke(tool_args)
    print(result)
```

## Agents

- Agent: Agente que pode tomar decisÃµes e executar aÃ§Ãµes com base em entradas e saÃ­das.

```mermaid
flowchart TB
    A[Agent] --> B[Tool]
    A --> C[Memoria]
    A --> D[Planejamento]
```

- tool: Tem que ter a capacidade de interagir com diversas ferramentas terceiras.
- memoria: Armazenar e recuperar informaÃ§Ã£o - aprende com experiÃªncias passadas.
- planejamento: Estruturar e analisar o problema e quebrando em n passos.

- LLM Ganhar a capacidade de `planning` e `reasoning` (raciocÃ­nio).

## Estudo de caso

```mermaid
flowchart LR
    A[Texto] --> B[Embedding]
    C[Video] --> c1[extrai audio]
    c1 --> c2[txt] --> B
    D[Foto] --> d1[Embedding multimodal]
    d1 --> B
    B --> E[Vector]
    E --> F[Banco de dados vetorial]
```

```mermaid
flowchart LR
    A[Prompt] --> B[Escrever Pergunta]
    B --> C[Embedding da pergunta]
    C --> D[Buscar por proximidade vetorial]
    E --> F[LLM]
```

- https://dropbox.tech/machine-learning/building-dash-rag-multi-step-ai-agents-business-users
- https://builders.ramp.com/post/industry_classification
- https://building.nubank.com/pt-br/solucao-de-ia-para-busca/
- https://www.shopify.com/blog/ai-models